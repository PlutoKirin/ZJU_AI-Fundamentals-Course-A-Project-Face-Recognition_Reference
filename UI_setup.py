import cv2
import dlib
import numpy as np
import gradio as gr
import pickle
import pandas as pd
from datetime import datetime
import os
import time
from scipy.spatial import distance as dist

# === é…ç½® ===
ATTENDANCE_FOLDER = "attendance_results"  # ç­¾åˆ°ç»“æœæ–‡ä»¶å¤¹
os.makedirs(ATTENDANCE_FOLDER, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹

# æ¨¡å‹è·¯å¾„ï¼ˆä¸è„šæœ¬åŒç›®å½•ï¼‰
SHAPE_PREDICTOR_PATH = "shape_predictor_68_face_landmarks.dat"
FACE_ENCODER_PATH = "dlib_face_recognition_resnet_model_v1.dat"

# è¯„ä¼°å›¾ç‰‡è·¯å¾„
EVALUATION_FOLDER = "evaluation"
CONFUSION_MATRIX_PATH = os.path.join(EVALUATION_FOLDER, "confusion_matrix.png")
METRICS_PATH = os.path.join(EVALUATION_FOLDER, "metrics.png")


# æ£€æŸ¥è¯„ä¼°å›¾ç‰‡æ˜¯å¦å­˜åœ¨
def check_evaluation_images():
    """æ£€æŸ¥è¯„ä¼°å›¾ç‰‡æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None"""
    images = {}
    for name, path in [("confusion_matrix", CONFUSION_MATRIX_PATH),
                       ("metrics", METRICS_PATH)]:
        if os.path.exists(path) and os.path.isfile(path):
            images[name] = path
        else:
            images[name] = None
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°è¯„ä¼°å›¾ç‰‡ - {path}")
    return images


# åŠ è½½æ¨¡å‹å’Œç‰¹å¾æ•°æ®
try:
    detector = dlib.get_frontal_face_detector()
    shape_predictor = dlib.shape_predictor(SHAPE_PREDICTOR_PATH)
    face_encoder = dlib.face_recognition_model_v1(FACE_ENCODER_PATH)
    print("æ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    raise

try:
    with open("face_features.pkl", "rb") as f:
        name_features = pickle.load(f)
    print(f"æˆåŠŸåŠ è½½ {len(name_features)} ä¸ªäººè„¸ç‰¹å¾")
except (FileNotFoundError, pickle.UnpicklingError) as e:
    name_features = {}
    print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½äººè„¸ç‰¹å¾æ•°æ®åº“ - {str(e)}")
    print("æ‰€æœ‰äººè„¸å°†è¢«è¯†åˆ«ä¸º'Unknown'")

# è®¾å®šé˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰
RECOGNITION_THRESHOLD = 0.45

# === ç­¾åˆ°ç³»ç»ŸçŠ¶æ€ ===
signed_names = set()  # å·²ç­¾åˆ°äººå‘˜é›†åˆ
attendance_records = []  # ç­¾åˆ°è®°å½•
is_signing_active = False  # ç­¾åˆ°ç³»ç»Ÿæ˜¯å¦æ¿€æ´»


# === å·¥å…·å‡½æ•° ===
def eye_aspect_ratio(eye):
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])
    return (A + B) / (2.0 * C)


def mouth_aspect_ratio(mouth):
    A = np.linalg.norm(mouth[2] - mouth[9])
    B = np.linalg.norm(mouth[4] - mouth[7])
    C = np.linalg.norm(mouth[0] - mouth[6])
    return (A + B) / (2.0 * C)


def nod_aspect_ratio(size, pre_point, now_point):
    return abs(float((pre_point[1] - now_point[1]) / (size[0] / 2)))


def shake_aspect_ratio(size, pre_point, now_point):
    return abs(float((pre_point[0] - now_point[0]) / (size[1] / 2)))


def reset_attendance():
    """é‡ç½®ç­¾åˆ°çŠ¶æ€"""
    global signed_names, attendance_records, is_signing_active
    signed_names = set()
    attendance_records = []
    is_signing_active = False
    return "ç­¾åˆ°çŠ¶æ€å·²é‡ç½®", ""


def save_attendance_to_excel():
    """ä¿å­˜ç­¾åˆ°è®°å½•åˆ°Excel"""
    if not attendance_records:
        return "æ²¡æœ‰ç­¾åˆ°è®°å½•å¯ä¿å­˜"

    df = pd.DataFrame(attendance_records, columns=["å§“å", "ç­¾åˆ°æ—¶é—´"])
    filename = f"ç­¾åˆ°è®°å½•_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    path = os.path.join(ATTENDANCE_FOLDER, filename)

    try:
        with pd.ExcelWriter(path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name="ç­¾åˆ°è¡¨")
        return f"ç­¾åˆ°ç»“æœå·²ä¿å­˜åˆ°ï¼š{path}"
    except Exception as e:
        return f"ä¿å­˜å¤±è´¥: {str(e)}"


def get_signed_names_text():
    """è·å–å·²ç­¾åˆ°äººå‘˜çš„æ–‡æœ¬åˆ—è¡¨"""
    if not signed_names:
        return "æš‚æ— ç­¾åˆ°è®°å½•"
    return "\n".join([f"{i + 1}. {name}" for i, name in enumerate(sorted(signed_names))])


# === äººè„¸è¯†åˆ«å‡½æ•° ===
def recognize_face(frame, threshold=RECOGNITION_THRESHOLD):
    """é€šç”¨äººè„¸è¯†åˆ«å‡½æ•°ï¼Œè¿”å›è¯†åˆ«ç»“æœå¹¶ä¿ç•™æ ‡æ³¨"""
    if frame is None or frame.size == 0:
        return frame, []

    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    faces = detector(rgb, 1)
    results = []

    for face in faces:
        shape = shape_predictor(rgb, face)
        descriptor = face_encoder.compute_face_descriptor(rgb, shape)
        current_feature = np.array(descriptor)

        identity = "Unknown"
        min_distance = float("inf")

        # éå†æ‰€æœ‰å·²çŸ¥äººè„¸ç‰¹å¾ï¼Œæ‰¾åˆ°è·ç¦»æœ€å°ä¸”å°äºé˜ˆå€¼çš„åŒ¹é…
        for name, saved_feature in name_features.items():
            distance = np.linalg.norm(current_feature - saved_feature)
            if distance < min_distance and distance < threshold:  # ä½¿ç”¨ä¼ å…¥çš„thresholdå‚æ•°
                min_distance = distance
                identity = name

        confidence = (1 - min_distance / threshold) * 100 if min_distance < threshold else 0

        results.append({
            "name": identity,
            "confidence": confidence,
            "location": (face.left(), face.top(), face.right(), face.bottom())
        })

        # ä¿ç•™äººè„¸è¯†åˆ«æ ‡æ³¨ï¼ˆå§“åå’Œç½®ä¿¡åº¦ï¼‰
        color = (0, 255, 0) if identity != "Unknown" else (0, 0, 255)
        cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), color, 2)
        try:
            cv2.putText(frame, f"{identity} ({confidence:.1f}%)",
                        (face.left() + 5, face.top() - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        except:
            cv2.putText(frame, f"{identity} ({confidence:.1f}%)",
                        (face.left() + 5, face.top() - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    return frame, results


def process_sign_in(frame, threshold=RECOGNITION_THRESHOLD):
    """ç­¾åˆ°å‡½æ•°ï¼Œç§»é™¤ç­¾åˆ°çŠ¶æ€æ ‡æ³¨"""
    global is_signing_active, signed_names, attendance_records

    if not is_signing_active or frame is None:
        return frame, get_signed_names_text()

    _, faces = recognize_face(frame.copy(), threshold)

    # å¤„ç†ç­¾åˆ°é€»è¾‘ï¼ˆä¿ç•™äººè„¸è¯†åˆ«ï¼Œä½†ç§»é™¤ç­¾åˆ°çŠ¶æ€æ–‡å­—æ ‡æ³¨ï¼‰
    for face in faces:
        if face["name"] != "Unknown" and face["name"] not in signed_names:
            signed_names.add(face["name"])
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            attendance_records.append([face["name"], now])
            print(f"[å·²ç­¾åˆ°] {face['name']} â€”â€” {now}")

    # ç§»é™¤ç­¾åˆ°çŠ¶æ€æ–‡å­—æ ‡æ³¨ï¼ˆä»…ä¿ç•™äººè„¸è¯†åˆ«æ ‡æ³¨ï¼‰
    # åŸä»£ç ä¸­çš„ cv2.putText å·²åˆ é™¤

    return frame, get_signed_names_text()


def recognize_face_image(image_path, threshold=RECOGNITION_THRESHOLD):
    """å›¾ç‰‡äººè„¸è¯†åˆ«"""
    if not image_path or not os.path.exists(image_path):
        return None, "é”™è¯¯ï¼šæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨"

    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    ext = os.path.splitext(image_path)[1].lower()
    if ext not in valid_extensions:
        return None, "é”™è¯¯ï¼šä»…æ”¯æŒJPG/PNG/BMPæ ¼å¼"

    try:
        frame = cv2.imread(image_path)
        if frame is None:
            return None, "é”™è¯¯ï¼šæ— æ•ˆå›¾ç‰‡æ–‡ä»¶"

        result_frame, faces = recognize_face(frame, threshold)
        if result_frame is not None:
            result_frame = cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB)

        if not faces:
            return result_frame, "æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·ç¡®ä¿äººè„¸æ¸…æ™°ä¸”å ç”»é¢æ¯”ä¾‹è¶³å¤Ÿ"

        report = "\n".join([
            f"{i + 1}. {face['name']} ({face['confidence']:.1f}%) - ä½ç½®: {face['location']}"
            for i, face in enumerate(faces)
        ])
        return result_frame, report

    except Exception as e:
        return None, f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™ï¼š{str(e)}"


def create_app():
    evaluation_images = check_evaluation_images()

    with gr.Blocks(title="äººè„¸è¯†åˆ«ç³»ç»Ÿ") as app:
        gr.Markdown("# äººè„¸è¯†åˆ«ç³»ç»Ÿ")

        with gr.Tabs():
            # 1. ç­¾åˆ°ç³»ç»Ÿé€‰é¡¹å¡
            with gr.Tab("ç­¾åˆ°ç³»ç»Ÿ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        threshold_slider = gr.Slider(0.1, 0.8, RECOGNITION_THRESHOLD, step=0.05, label="è¯†åˆ«é˜ˆå€¼")
                        status_text = gr.Textbox("å°±ç»ª", label="ç³»ç»ŸçŠ¶æ€", lines=2)
                        start_sign_btn = gr.Button("å¼€å§‹ç­¾åˆ°", variant="primary")
                        stop_sign_btn = gr.Button("åœæ­¢ç­¾åˆ°", variant="secondary", interactive=False)
                        export_btn = gr.Button("å¯¼å‡ºç­¾åˆ°ç»“æœ", variant="info", interactive=False)
                        reset_btn = gr.Button("é‡ç½®ç­¾åˆ°", variant="secondary")
                        export_status = gr.Textbox("", label="å¯¼å‡ºçŠ¶æ€", lines=1)
                        gr.Markdown("### å·²ç­¾åˆ°äººå‘˜")
                        signed_names_display = gr.Textbox("", lines=10, interactive=False)
                    with gr.Column(scale=2):
                        webcam = gr.Image(source="webcam", streaming=True, type="numpy", label="æ‘„åƒå¤´ç”»é¢")
                        output = gr.Image(label="ç­¾åˆ°ç»“æœ", type="numpy")

                def start_signing_fn():
                    global is_signing_active
                    is_signing_active = True
                    return (
                        gr.Button("å¼€å§‹ç­¾åˆ°", variant="primary", interactive=False),
                        gr.Button("åœæ­¢ç­¾åˆ°", variant="secondary", interactive=True),
                        gr.Button("å¯¼å‡ºç­¾åˆ°ç»“æœ", variant="info", interactive=True),
                        "ç­¾åˆ°å·²å¯åŠ¨"
                    )

                def stop_signing_fn():
                    global is_signing_active
                    is_signing_active = False
                    return (
                        gr.Button("å¼€å§‹ç­¾åˆ°", variant="primary", interactive=True),
                        gr.Button("åœæ­¢ç­¾åˆ°", variant="secondary", interactive=False),
                        "ç­¾åˆ°å·²åœæ­¢",
                        get_signed_names_text()
                    )

                start_sign_btn.click(start_signing_fn, outputs=[start_sign_btn, stop_sign_btn, export_btn, status_text])
                stop_sign_btn.click(stop_signing_fn,
                                    outputs=[start_sign_btn, stop_sign_btn, status_text, signed_names_display])
                export_btn.click(save_attendance_to_excel, outputs=export_status)
                reset_btn.click(reset_attendance, outputs=[status_text, signed_names_display])
                webcam.stream(process_sign_in, inputs=[webcam, threshold_slider],
                              outputs=[output, signed_names_display])

            # 2. è¯¦ç»†è¯†åˆ«ç»“æœé€‰é¡¹å¡
            with gr.Tab("è¯¦ç»†è¯†åˆ«ç»“æœ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        threshold_slider_detail = gr.Slider(0.1, 0.8, RECOGNITION_THRESHOLD, step=0.05,
                                                            label="è¯†åˆ«é˜ˆå€¼")
                        show_fps = gr.Checkbox(False, label="æ˜¾ç¤ºå¸§ç‡")
                        status_text_detail = gr.Textbox("å°±ç»ª", label="ç³»ç»ŸçŠ¶æ€", lines=2)
                        detected_faces = gr.Textbox("æœªæ£€æµ‹åˆ°äººè„¸", label="è¯†åˆ«ç»“æœ", lines=5)
                        face_boxes = gr.Textbox("", label="äººè„¸æ¡†ä½ç½®", lines=5)
                        start_btn = gr.Button("å¯åŠ¨è¯†åˆ«", variant="primary")
                        stop_btn = gr.Button("åœæ­¢è¯†åˆ«", variant="secondary", interactive=False)
                    with gr.Column(scale=2):
                        webcam_detail = gr.Image(source="webcam", streaming=True, type="numpy", label="æ‘„åƒå¤´ç”»é¢")
                        output_detail = gr.Image(label="è¯†åˆ«ç»“æœ", type="numpy")

                is_recognizing = gr.State(False)  # ç‹¬ç«‹çš„è¯†åˆ«çŠ¶æ€
                last_time = gr.State(time.time())

                def start_recognition_fn():
                    is_recognizing.value = True
                    last_time.value = time.time()
                    return (
                        gr.Button("å¯åŠ¨è¯†åˆ«", variant="primary", interactive=False),
                        gr.Button("åœæ­¢è¯†åˆ«", variant="secondary", interactive=True),
                        "æ­£åœ¨è¯†åˆ«..."
                    )

                def stop_recognition_fn():
                    is_recognizing.value = False
                    return (
                        gr.Button("å¯åŠ¨è¯†åˆ«", variant="primary", interactive=True),
                        gr.Button("åœæ­¢è¯†åˆ«", variant="secondary", interactive=False),
                        "å·²åœæ­¢"
                    )

                def process_frame_detail_fn(frame, threshold_val, show_fps_val):
                    if not is_recognizing.value or frame is None:
                        return frame, "æœªæ£€æµ‹åˆ°äººè„¸", ""
                    result_image, results = recognize_face(frame, threshold_val)
                    text_result = "\n".join([f"{i + 1}. {r['name']} ({r['confidence']:.1f}%)" for i, r in
                                             enumerate(results)]) if results else "æœªæ£€æµ‹åˆ°äººè„¸"
                    box_result = "\n".join(
                        [f"{i + 1}. {r['location']}" for i, r in enumerate(results)]) if results else ""
                    if show_fps_val:
                        current_time = time.time()
                        fps = 1 / (current_time - last_time.value) if (current_time - last_time.value) > 0 else 0
                        last_time.value = current_time
                        cv2.putText(result_image, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                                    (0, 255, 0), 2)
                    return result_image, text_result, box_result

                start_btn.click(start_recognition_fn, outputs=[start_btn, stop_btn, status_text_detail])
                stop_btn.click(stop_recognition_fn, outputs=[start_btn, stop_btn, status_text_detail])
                webcam_detail.stream(process_frame_detail_fn, inputs=[webcam_detail, threshold_slider_detail, show_fps],
                                     outputs=[output_detail, detected_faces, face_boxes])

            # 3. å›¾ç‰‡äººè„¸è¯†åˆ«é€‰é¡¹å¡
            with gr.Tab("å›¾ç‰‡äººè„¸è¯†åˆ«"):
                with gr.Column():
                    gr.Markdown("# å›¾ç‰‡äººè„¸è¯†åˆ«")
                    with gr.Row():
                        with gr.Column(scale=1):
                            image_input = gr.File(label="ä¸Šä¼ å›¾ç‰‡", file_types=["image"])
                            threshold_img_slider = gr.Slider(0.1, 0.8, RECOGNITION_THRESHOLD, step=0.05,
                                                             label="è¯†åˆ«é˜ˆå€¼")
                            recognize_btn = gr.Button("å¼€å§‹è¯†åˆ«", variant="primary")
                            result_text = gr.Textbox("", label="è¯†åˆ«æŠ¥å‘Š", lines=10)
                        with gr.Column(scale=2):
                            result_image = gr.Image(label="è¯†åˆ«ç»“æœ", type="numpy")

                    def process_image_fn(file, threshold):
                        if not file:
                            return None, "è¯·ä¸Šä¼ å›¾ç‰‡"
                        return recognize_face_image(file.name, threshold)

                    recognize_btn.click(process_image_fn, inputs=[image_input, threshold_img_slider],
                                        outputs=[result_image, result_text])

            # 4. æ´»ä½“æ£€æµ‹é€‰é¡¹å¡
            with gr.Tab("æ´»ä½“æ£€æµ‹"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ£€æµ‹æŒ‡å¼•")
                        gr.Markdown("- çœ¨çœ¼ï¼ˆè¿ç»­é—­çœ¼3æ¬¡ï¼‰")
                        gr.Markdown("- å¼ å˜´ï¼ˆä¿æŒå¼€å£5ç§’ï¼‰")
                        gr.Markdown("- ç‚¹å¤´/æ‘‡å¤´ï¼ˆä»»æ„åŠ¨ä½œ5æ¬¡ï¼‰")
                        live_status = gr.Textbox("", label="æ£€æµ‹çŠ¶æ€", lines=3, interactive=False)
                        action_results = gr.Textbox("", label="åŠ¨ä½œè¯¦æƒ…", lines=4, interactive=False)
                        start_live_btn = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
                        stop_live_btn = gr.Button("åœæ­¢æ£€æµ‹", variant="secondary", interactive=False)
                        reset_live_btn = gr.Button("é‡ç½®æ£€æµ‹", variant="secondary")
                    with gr.Column(scale=2):
                        live_webcam = gr.Image(source="webcam", streaming=True, type="numpy", label="å®æ—¶ç”»é¢")
                        live_output = gr.Image(label="æ£€æµ‹ç»“æœ", type="numpy")

                # çŠ¶æ€å˜é‡
                is_live_active = gr.State(False)
                detection_passed = gr.State(False)
                blink_cnt = gr.State(0)
                mouth_cnt = gr.State(0)
                head_cnt = gr.State(0)
                success_count = gr.State(0)
                compare_point = gr.State([0, 0])
                size = gr.State((480, 640))

                def live_detection(frame):
                    if not is_live_active.value or frame is None:
                        if detection_passed.value:
                            return frame, "âœ… æ´»ä½“æ£€æµ‹é€šè¿‡ï¼", "è¯·ç‚¹å‡»'é‡ç½®æ£€æµ‹'é‡æ–°å¼€å§‹"
                        return frame, "", ""

                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    faces = detector(frame_rgb, 0)
                    size.value = frame.shape[:2]

                    if len(faces) != 1:
                        compare_point.value = [0, 0]
                        return frame, "è¯·ç¡®ä¿ä»…1äººæ­£å¯¹æ‘„åƒå¤´", ""

                    face = faces[0]
                    landmarks = np.array([[p.x, p.y] for p in shape_predictor(frame_rgb, face).parts()])
                    action_details = []

                    # çœ¨çœ¼æ£€æµ‹
                    left_eye = landmarks[36:42]
                    right_eye = landmarks[42:48]
                    ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2
                    if ear < 0.2:
                        blink_cnt.value += 1
                        if blink_cnt.value >= 3:
                            blink_status = "âœ… çœ¨çœ¼æˆåŠŸ"
                            if success_count.value < 3:
                                success_count.value += 1
                            blink_cnt.value = 0
                        else:
                            blink_status = f"â³ çœ¨çœ¼è¿›åº¦: {blink_cnt.value}/3"
                    else:
                        blink_status = "ğŸ”µ ç­‰å¾…çœ¨çœ¼"
                    action_details.append(blink_status)

                    # å¼ å˜´æ£€æµ‹
                    mouth = landmarks[48:68]
                    mar = mouth_aspect_ratio(mouth)
                    if mar > 0.5:
                        mouth_cnt.value += 1
                        if mouth_cnt.value >= 5:
                            mouth_status = "âœ… å¼ å˜´æˆåŠŸ"
                            if success_count.value < 3:
                                success_count.value += 1
                            mouth_cnt.value = 0
                        else:
                            mouth_status = f"â³ å¼ å˜´è¿›åº¦: {mouth_cnt.value}/5"
                    else:
                        mouth_status = "ğŸ”µ ç­‰å¾…å¼ å˜´"
                    action_details.append(mouth_status)

                    # å¤´éƒ¨åŠ¨ä½œæ£€æµ‹
                    nose_points = landmarks[27:36]
                    nose_center = nose_points.mean(axis=0).tolist()
                    prev_point = compare_point.value if compare_point.value else [0, 0]

                    if prev_point != [0, 0]:
                        nod_val = nod_aspect_ratio(size.value, prev_point, nose_center)
                        shake_val = shake_aspect_ratio(size.value, prev_point, nose_center)
                        if nod_val > 0.03 or shake_val > 0.03:
                            head_cnt.value += 1
                            if head_cnt.value >= 5:
                                head_status = "âœ… å¤´éƒ¨åŠ¨ä½œæˆåŠŸ"
                                if success_count.value < 3:
                                    success_count.value += 1
                                head_cnt.value = 0
                            else:
                                head_status = f"â³ å¤´éƒ¨åŠ¨ä½œè¿›åº¦: {head_cnt.value}/5"
                        else:
                            head_status = "ğŸ”µ ç­‰å¾…å¤´éƒ¨åŠ¨ä½œ"
                    else:
                        head_status = "ğŸ”µ ç­‰å¾…å¤´éƒ¨åŠ¨ä½œ"
                    action_details.append(head_status)

                    # ç»¼åˆç»“æœ
                    live_status_msg = "è¿›è¡Œä¸­... éœ€è¦å®Œæˆ2ç§åŠ¨ä½œ"
                    if success_count.value >= 2:
                        live_status_msg = "âœ… æ´»ä½“æ£€æµ‹é€šè¿‡ï¼"
                        detection_passed.value = True
                        is_live_active.value = False
                        blink_cnt.value = mouth_cnt.value = head_cnt.value = 0
                        success_count.value = 0

                    action_details_text = "\n".join(action_details)

                    # ä»…ä¿ç•™äººè„¸æ¡†æ ‡æ³¨ï¼Œç§»é™¤æ‰€æœ‰æ–‡å­—æ ‡æ³¨
                    cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (0, 255, 0), 2)
                    # ç§»é™¤ cv2.putText è°ƒç”¨ï¼Œä¸å†æ˜¾ç¤ºçŠ¶æ€æ–‡å­—

                    compare_point.value = nose_center
                    return frame, live_status_msg, action_details_text

                def start_live():
                    is_live_active.value = True
                    detection_passed.value = False
                    blink_cnt.value = 0
                    mouth_cnt.value = 0
                    head_cnt.value = 0
                    success_count.value = 0
                    compare_point.value = [0, 0]
                    return (
                        gr.Button("å¼€å§‹æ£€æµ‹", variant="primary", interactive=False),
                        gr.Button("åœæ­¢æ£€æµ‹", variant="secondary", interactive=True),
                        "æ£€æµ‹å·²å¯åŠ¨",
                        ""
                    )

                def stop_live():
                    is_live_active.value = False
                    return (
                        gr.Button("å¼€å§‹æ£€æµ‹", variant="primary", interactive=True),
                        gr.Button("åœæ­¢æ£€æµ‹", variant="secondary", interactive=False),
                        "æ£€æµ‹å·²åœæ­¢",
                        ""
                    )

                def reset_live():
                    is_live_active.value = False
                    detection_passed.value = False
                    blink_cnt.value = 0
                    mouth_cnt.value = 0
                    head_cnt.value = 0
                    success_count.value = 0
                    compare_point.value = [0, 0]
                    return "æ£€æµ‹çŠ¶æ€å·²é‡ç½®", ""

                start_live_btn.click(
                    start_live,
                    outputs=[start_live_btn, stop_live_btn, live_status, action_results]
                )
                stop_live_btn.click(
                    stop_live,
                    outputs=[start_live_btn, stop_live_btn, live_status, action_results]
                )
                reset_live_btn.click(reset_live, outputs=[live_status, action_results])
                live_webcam.stream(
                    live_detection,
                    inputs=[live_webcam],
                    outputs=[live_output, live_status, action_results]
                )

            # 5. ç³»ç»Ÿå‚æ•°è®¾ç½®ä»‹ç»
            with gr.Tab("ç³»ç»Ÿå‚æ•°è®¾ç½®"):
                gr.Markdown("# ç³»ç»Ÿå‚æ•°è®¾ç½®è¯´æ˜")

                # æ˜¾ç¤ºä¸¤å¼ è¯„ä¼°å›¾ç‰‡
                gr.Markdown("## ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡")
                with gr.Row():
                    with gr.Column():
                        metrics_image = gr.Image(
                            value=evaluation_images["metrics"],
                            label="ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡",
                            type="filepath",
                            interactive=False
                        )
                    with gr.Column():
                        confusion_matrix = gr.Image(
                            value=evaluation_images["confusion_matrix"],
                            label="æ··æ·†çŸ©é˜µ",
                            type="filepath",
                            interactive=False
                        )

                # è¶…å‚æ•°è®¾ç½®
                gr.Markdown("## è¶…å‚æ•°è®¾ç½®")
                gr.Markdown("""
                ç³»ç»Ÿæä¾›äº†ä»¥ä¸‹å¯è°ƒæ•´çš„è¶…å‚æ•°ï¼Œç”¨æˆ·å¯æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼š

                ### äººè„¸è¯†åˆ«é˜ˆå€¼ (`RECOGNITION_THRESHOLD`)
                - **é»˜è®¤å€¼**ï¼š0.45
                - **èŒƒå›´**ï¼š0.1 - 0.8
                - **å½±å“**ï¼š
                  - é˜ˆå€¼è¶Šå°ï¼Œè¯†åˆ«è¦æ±‚è¶Šä¸¥æ ¼ï¼Œè¯¯è¯†ç‡é™ä½ï¼Œä½†å¯èƒ½å¢åŠ æ‹’è¯†ç‡
                  - é˜ˆå€¼è¶Šå¤§ï¼Œè¯†åˆ«è¦æ±‚è¶Šå®½æ¾ï¼Œè¯†åˆ«ç‡æé«˜ï¼Œä½†å¯èƒ½å¢åŠ è¯¯è¯†ç‡
                - **è°ƒæ•´å»ºè®®**ï¼š
                  - åœ¨å®‰å…¨è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸‹ï¼Œå¯é€‚å½“é™ä½é˜ˆå€¼ï¼ˆå¦‚0.4ï¼‰
                  - åœ¨éœ€è¦é«˜è¯†åˆ«ç‡çš„åœºæ™¯ä¸‹ï¼Œå¯é€‚å½“æé«˜é˜ˆå€¼ï¼ˆå¦‚0.5ï¼‰

                ### æ´»ä½“æ£€æµ‹å‚æ•°
                - **çœ¨çœ¼æ£€æµ‹**ï¼š
                  - çœ¼ç›çºµæ¨ªæ¯”é˜ˆå€¼ï¼š0.2
                  - è§¦å‘æ¬¡æ•°ï¼šè¿ç»­çœ¨çœ¼3æ¬¡
                - **å¼ å˜´æ£€æµ‹**ï¼š
                  - å˜´å·´çºµæ¨ªæ¯”é˜ˆå€¼ï¼š0.5
                  - è§¦å‘æŒç»­æ—¶é—´ï¼šä¿æŒå¼ å˜´5ç§’
                - **å¤´éƒ¨åŠ¨ä½œæ£€æµ‹**ï¼š
                  - ç‚¹å¤´/æ‘‡å¤´é˜ˆå€¼ï¼š0.03
                  - è§¦å‘æ¬¡æ•°ï¼šä»»æ„åŠ¨ä½œ5æ¬¡

                ### ç³»ç»Ÿæ€§èƒ½å‚æ•°
                - **æ‘„åƒå¤´å¸§ç‡æ§åˆ¶**ï¼š
                  - é»˜è®¤å¯ç”¨è‡ªé€‚åº”å¸§ç‡
                  - å¯é€šè¿‡ç•Œé¢å¤é€‰æ¡†å¯ç”¨FPSæ˜¾ç¤º
                """)

                # æ¨¡å‹å‚æ•°è®¾ç½®
                gr.Markdown("## æ¨¡å‹å‚æ•°è®¾ç½®")
                gr.Markdown("""
                ç³»ç»Ÿä½¿ç”¨çš„ä¸»è¦æ¨¡å‹åŠå…¶å‚æ•°é…ç½®ï¼š

                ### äººè„¸æ£€æµ‹å™¨ (`dlib.get_frontal_face_detector()`)
                - **æ¨¡å‹ç±»å‹**ï¼šåŸºäºHOGç‰¹å¾å’ŒSVMçš„äººè„¸æ£€æµ‹å™¨
                - **ç‰¹ç‚¹**ï¼š
                  - å¯¹æ­£é¢å’Œè¿‘æ­£é¢äººè„¸æ£€æµ‹æ•ˆæœè¾ƒå¥½
                  - è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå®æ—¶åº”ç”¨
                - **å‚æ•°è°ƒæ•´**ï¼š
                  - æ£€æµ‹å™¨ç¼©æ”¾å› å­ï¼šé»˜è®¤å€¼ä¸º1ï¼ˆå¯é€šè¿‡`detector(rgb, 1)`ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°è°ƒæ•´ï¼‰

                ### äººè„¸ç‰¹å¾æå–å™¨ (`dlib.face_recognition_model_v1`)
                - **æ¨¡å‹ç±»å‹**ï¼šåŸºäºResNetçš„128ç»´äººè„¸ç‰¹å¾æå–å™¨
                - **ç‰¹ç‚¹**ï¼š
                  - æå–çš„ç‰¹å¾å…·æœ‰è¾ƒé«˜çš„åŒºåˆ†æ€§
                  - å¯¹å…‰ç…§ã€è¡¨æƒ…å˜åŒ–æœ‰è¾ƒå¥½çš„é²æ£’æ€§
                - **ä¾èµ–æ–‡ä»¶**ï¼š
                  - `shape_predictor_68_face_landmarks.dat`ï¼š68ç‚¹äººè„¸ç‰¹å¾ç‚¹æ£€æµ‹å™¨
                  - `dlib_face_recognition_resnet_model_v1.dat`ï¼šäººè„¸ç‰¹å¾æå–æ¨¡å‹

                ### ç‰¹å¾åŒ¹é…æ–¹æ³•
                - **è·ç¦»åº¦é‡**ï¼šæ¬§æ°è·ç¦»
                - **åŒ¹é…é€»è¾‘**ï¼š
                  - è®¡ç®—è¾“å…¥äººè„¸ç‰¹å¾ä¸æ•°æ®åº“ä¸­ç‰¹å¾çš„æ¬§æ°è·ç¦»
                  - è·ç¦»å°äºè®¾å®šé˜ˆå€¼çš„è®¤ä¸ºåŒ¹é…æˆåŠŸ
                """)

                # ä»…ä¿ç•™å‚æ•°è°ƒæ•´æ­¥éª¤
                gr.Markdown("## å‚æ•°è°ƒæ•´æŒ‡å—")
                gr.Markdown("""
                ### è°ƒæ•´æ­¥éª¤
                1. åœ¨ç›¸åº”åŠŸèƒ½ç•Œé¢ä¸­æ‰¾åˆ°å‚æ•°è°ƒæ•´æ»‘å—æˆ–é€‰é¡¹
                2. é€æ­¥è°ƒæ•´å‚æ•°å¹¶è§‚å¯Ÿç³»ç»Ÿè¡¨ç°
                3. ä¿å­˜æœ€ä½³å‚æ•°é…ç½®
                """)

    return app


if __name__ == "__main__":
    app = create_app()
    app.launch(share=False)